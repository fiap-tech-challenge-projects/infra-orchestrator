name: Destroy All Infrastructure

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to destroy'
        required: true
        type: choice
        options:
          - staging
          - production
      confirm:
        description: 'Type "DESTROY" to confirm'
        required: true
        type: string

env:
  AWS_REGION: 'us-east-1'

jobs:
  # ---------------------------------------------------------------------------
  # Validate Confirmation
  # ---------------------------------------------------------------------------
  validate:
    name: Validate Confirmation
    runs-on: ubuntu-latest
    steps:
      - name: Check Confirmation
        if: ${{ github.event.inputs.confirm != 'DESTROY' }}
        run: |
          echo "::error::You must type 'DESTROY' to confirm destruction"
          exit 1

      - name: Warning
        run: |
          echo "::warning::You are about to destroy ALL infrastructure in ${{ github.event.inputs.environment }}"
          echo "## DESTRUCTION CONFIRMED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Environment: **${{ github.event.inputs.environment }}**" >> $GITHUB_STEP_SUMMARY

  # ---------------------------------------------------------------------------
  # Clean Terraform State Locks
  # ---------------------------------------------------------------------------
  clean-locks:
    name: Clean Terraform State Locks
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # AWS_SESSION_TOKEN is optional (only required for AWS Academy)
          # If not set, it will be empty and the action will ignore it
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Clean state locks
        run: |
          echo "Checking for stale Terraform state locks..."

          # Scan for locks
          locks=$(aws dynamodb scan --table-name fiap-terraform-locks --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")

          if [ -n "$locks" ]; then
            echo "Found locks to clean:"
            echo "$locks"

            # Delete each lock
            for lock in $locks; do
              if [[ "$lock" == *"${{ github.event.inputs.environment }}"* ]]; then
                echo "Deleting lock: $lock"
                aws dynamodb delete-item \
                  --table-name fiap-terraform-locks \
                  --key "{\"LockID\": {\"S\": \"$lock\"}}" 2>/dev/null || true
              fi
            done

            echo "✓ Locks cleaned"
          else
            echo "No locks found"
          fi

  # ---------------------------------------------------------------------------
  # Step 1: Destroy K8s Application (reverse order)
  # ---------------------------------------------------------------------------
  destroy-app:
    name: "1. Destroy K8s Application"
    runs-on: ubuntu-latest
    needs: [validate, clean-locks]
    continue-on-error: true
    steps:
      - name: Trigger k8s-main-service destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'k8s-main-service',
                workflow_id: 'deploy.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger k8s-main-service destroy:', error.message);
            }

  # ---------------------------------------------------------------------------
  # Step 2: Destroy Lambda Functions
  # ---------------------------------------------------------------------------
  destroy-lambda:
    name: "2. Destroy Lambda Functions"
    runs-on: ubuntu-latest
    needs: [validate, destroy-app]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger lambda-api-handler destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: 'lambda-api-handler',
              workflow_id: 'deploy.yml',
              ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
              inputs: {
                environment: '${{ github.event.inputs.environment }}',
                action: 'destroy'
              }
            });

      - name: Wait for Lambda destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            let attempts = 0;
            while (attempts < 20) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'lambda-api-handler',
                workflow_id: 'deploy.yml',
                per_page: 1
              });
              const latestRun = runs.data.workflow_runs[0];
              if (latestRun?.status === 'completed') {
                if (latestRun.conclusion === 'success') {
                  console.log('✓ Lambda destruction completed successfully');
                  return;
                } else {
                  console.log(`⚠ Lambda destruction failed: ${latestRun.conclusion}`);
                  return; // Don't throw, just log and continue
                }
              }
              attempts++;
              await delay(30000);
            }
            console.log('⚠ Lambda destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 3: Destroy Database
  # ---------------------------------------------------------------------------
  destroy-database:
    name: "3. Destroy Database"
    runs-on: ubuntu-latest
    needs: [validate, destroy-lambda]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger database-managed-infra destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: 'database-managed-infra',
              workflow_id: 'terraform.yml',
              ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
              inputs: {
                environment: '${{ github.event.inputs.environment }}',
                action: 'destroy'
              }
            });

      - name: Wait for Database destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            let attempts = 0;
            while (attempts < 30) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'database-managed-infra',
                workflow_id: 'terraform.yml',
                per_page: 1
              });
              const latestRun = runs.data.workflow_runs[0];
              if (latestRun?.status === 'completed') {
                if (latestRun.conclusion === 'success') {
                  console.log('✓ Database destruction completed successfully');
                  return;
                } else {
                  console.log(`⚠ Database destruction failed: ${latestRun.conclusion}`);
                  return; // Don't throw, just log and continue
                }
              }
              attempts++;
              await delay(30000);
            }
            console.log('⚠ Database destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 4a: Destroy K8s Addons (Phase 2)
  # ---------------------------------------------------------------------------
  destroy-eks-addons:
    name: "4a. Destroy K8s Addons (Phase 2)"
    runs-on: ubuntu-latest
    needs: [validate, destroy-database]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger kubernetes-addons destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: 'kubernetes-addons',
              workflow_id: 'terraform.yml',
              ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
              inputs: {
                environment: '${{ github.event.inputs.environment }}',
                action: 'destroy'
              }
            });

      - name: Wait for K8s addons destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            let attempts = 0;
            while (attempts < 20) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'kubernetes-addons',
                workflow_id: 'terraform.yml',
                per_page: 1
              });
              const latestRun = runs.data.workflow_runs[0];
              if (latestRun?.status === 'completed') {
                if (latestRun.conclusion === 'success') {
                  console.log('✓ K8s addons destruction completed successfully');
                  return;
                } else {
                  console.log(`⚠ K8s addons destruction failed: ${latestRun.conclusion}`);
                  return; // Don't throw, just log and continue
                }
              }
              attempts++;
              await delay(30000);
            }
            console.log('⚠ K8s addons destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 4a.5: Clean Kubernetes Resources (before cluster destruction)
  # ---------------------------------------------------------------------------
  clean-k8s-resources:
    name: "4a.5. Clean Kubernetes Resources"
    runs-on: ubuntu-latest
    needs: [validate, destroy-eks-addons]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Checkout k8s-main-service
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/k8s-main-service
          ref: ${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}
          token: ${{ secrets.GH_PAT }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Configure kubectl
        continue-on-error: true
        run: |
          aws eks update-kubeconfig \
            --region us-east-1 \
            --name fiap-tech-challenge-eks-${{ github.event.inputs.environment }}

      - name: Delete Kubernetes application resources
        continue-on-error: true
        run: |
          echo "=== Deleting k8s-main-service resources ==="

          # Delete via kustomize (deployment, service, ingress, serviceaccount, etc.)
          kubectl delete -k k8s/overlays/${{ github.event.inputs.environment }} \
            --ignore-not-found=true \
            --timeout=120s || true

          echo "✓ Application resources deleted"

      - name: Delete ExternalSecrets and SecretStore
        continue-on-error: true
        run: |
          echo "=== Deleting ExternalSecrets and SecretStore ==="

          namespace="ftc-app-${{ github.event.inputs.environment }}"

          # Delete ExternalSecrets first (to stop secret sync)
          kubectl delete externalsecrets --all -n "$namespace" --timeout=30s || true

          # Delete SecretStore
          kubectl delete secretstore --all -n "$namespace" --timeout=30s || true

          # Delete generated secrets
          kubectl delete secret fiap-tech-challenge-db-secrets -n "$namespace" --ignore-not-found=true || true
          kubectl delete secret fiap-tech-challenge-auth-config -n "$namespace" --ignore-not-found=true || true

          echo "✓ ExternalSecrets and SecretStore deleted"

      - name: Delete application namespaces
        continue-on-error: true
        run: |
          echo "=== Deleting application namespaces ==="

          # Delete staging/production namespace
          kubectl delete namespace ftc-app-${{ github.event.inputs.environment }} \
            --ignore-not-found=true \
            --timeout=120s || true

          echo "✓ Namespaces deleted"

      - name: Wait for finalizers
        continue-on-error: true
        run: |
          echo "Waiting 30s for Kubernetes finalizers to complete..."
          sleep 30

  # ---------------------------------------------------------------------------
  # Step 4b: Destroy EKS Cluster (Phase 1)
  # ---------------------------------------------------------------------------
  destroy-eks-cluster:
    name: "4b. Destroy EKS Cluster (Phase 1)"
    runs-on: ubuntu-latest
    needs: [validate, destroy-database, destroy-eks-addons, clean-k8s-resources]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger kubernetes-core-infra destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: 'kubernetes-core-infra',
              workflow_id: 'terraform.yml',
              ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
              inputs: {
                environment: '${{ github.event.inputs.environment }}',
                action: 'destroy'
              }
            });

      - name: Wait for EKS cluster destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            let attempts = 0;
            while (attempts < 60) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'kubernetes-core-infra',
                workflow_id: 'terraform.yml',
                per_page: 1
              });
              const latestRun = runs.data.workflow_runs[0];
              if (latestRun?.status === 'completed') {
                if (latestRun.conclusion === 'success') {
                  console.log('✓ EKS cluster destruction completed successfully');
                  return;
                } else {
                  console.log(`⚠ EKS cluster destruction failed: ${latestRun.conclusion}`);
                  return; // Don't throw, just log and continue
                }
              }
              attempts++;
              await delay(30000);
            }
            console.log('⚠ EKS cluster destruction timed out');

  # ---------------------------------------------------------------------------
  # Force Cleanup - Aggressive orphan resource cleanup
  # ---------------------------------------------------------------------------
  force-cleanup:
    name: Force Cleanup Orphaned Resources
    runs-on: ubuntu-latest
    needs: [destroy-app, destroy-lambda, destroy-database, destroy-eks-addons, destroy-eks-cluster]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install dependencies
        run: |
          sudo apt-get update -qq && sudo apt-get install -y jq > /dev/null 2>&1

      - name: Run aggressive cleanup script
        continue-on-error: true
        run: |
          chmod +x scripts/cleanup-aws-resources.sh
          echo "DELETE" | ./scripts/cleanup-aws-resources.sh ${{ github.event.inputs.environment }} || true

      - name: Force delete orphaned IAM policies
        continue-on-error: true
        run: |
          echo "=== Force deleting orphaned IAM policies ==="

          # List all custom policies with project name
          policies=$(aws iam list-policies --scope Local \
            --query "Policies[?contains(PolicyName, 'fiap-tech-challenge')].Arn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$policies" ]; then
            for policy_arn in $policies; do
              policy_name=$(echo "$policy_arn" | awk -F'/' '{print $NF}')
              echo "Processing policy: $policy_name"

              # Detach from all roles
              attached_roles=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter Role --query 'PolicyRoles[*].RoleName' --output text 2>/dev/null || echo "")
              for role in $attached_roles; do
                echo "  Detaching from role: $role"
                aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Detach from all users
              attached_users=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter User --query 'PolicyUsers[*].UserName' --output text 2>/dev/null || echo "")
              for user in $attached_users; do
                echo "  Detaching from user: $user"
                aws iam detach-user-policy --user-name "$user" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Detach from all groups
              attached_groups=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter Group --query 'PolicyGroups[*].GroupName' --output text 2>/dev/null || echo "")
              for group in $attached_groups; do
                echo "  Detaching from group: $group"
                aws iam detach-group-policy --group-name "$group" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Delete all non-default versions
              versions=$(aws iam list-policy-versions --policy-arn "$policy_arn" \
                --query 'Versions[?!IsDefaultVersion].VersionId' --output text 2>/dev/null || echo "")
              for version in $versions; do
                aws iam delete-policy-version --policy-arn "$policy_arn" --version-id "$version" 2>/dev/null || true
              done

              # Delete the policy
              echo "  Deleting policy: $policy_name"
              aws iam delete-policy --policy-arn "$policy_arn" 2>/dev/null || true
            done
            echo "✓ IAM policies cleaned"
          else
            echo "No orphaned policies found"
          fi

  # ---------------------------------------------------------------------------
  # Validate Resources Deleted
  # ---------------------------------------------------------------------------
  validate-cleanup:
    name: Validate Resources Deleted
    runs-on: ubuntu-latest
    needs: [force-cleanup]
    if: always()
    continue-on-error: true
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install jq
        run: |
          sudo apt-get update -qq && sudo apt-get install -y jq > /dev/null 2>&1

      - name: Final validation
        run: |
          echo "=== Final Validation ==="
          echo ""

          ORPHANED=0
          WARNINGS=""

          # Check ECR
          echo "Checking ECR repositories..."
          if ecr_repos=$(aws ecr describe-repositories --region us-east-1 2>/dev/null | jq -r '.repositories[].repositoryName' | grep "fiap-tech-challenge" 2>/dev/null); then
            echo "::warning::ECR repositories still exist: $ecr_repos"
            WARNINGS="${WARNINGS}\n- ECR repositories: $ecr_repos"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ ECR clean"
          fi

          # Check EKS
          echo "Checking EKS cluster..."
          if aws eks describe-cluster --name fiap-tech-challenge-eks-${{ github.event.inputs.environment }} --region us-east-1 &>/dev/null; then
            echo "::warning::EKS cluster still exists: fiap-tech-challenge-eks-${{ github.event.inputs.environment }}"
            WARNINGS="${WARNINGS}\n- EKS cluster: fiap-tech-challenge-eks-${{ github.event.inputs.environment }}"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ EKS clean"
          fi

          # Check RDS
          echo "Checking RDS instances..."
          if aws rds describe-db-instances --db-instance-identifier fiap-tech-challenge-db-${{ github.event.inputs.environment }} --region us-east-1 &>/dev/null; then
            echo "::warning::RDS still exists: fiap-tech-challenge-db-${{ github.event.inputs.environment }}"
            WARNINGS="${WARNINGS}\n- RDS: fiap-tech-challenge-db-${{ github.event.inputs.environment }}"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ RDS clean"
          fi

          # Check IAM Roles
          echo "Checking IAM roles..."
          if iam_roles=$(aws iam list-roles 2>/dev/null | jq -r '.Roles[].RoleName' | grep "fiap-tech-challenge" 2>/dev/null); then
            echo "::warning::IAM roles still exist"
            WARNINGS="${WARNINGS}\n- IAM roles: $(echo $iam_roles | head -n 3)"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ IAM clean"
          fi

          # Check VPC
          echo "Checking VPCs..."
          vpc_id=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Project,Values=fiap-tech-challenge" \
            --region us-east-1 \
            --query 'Vpcs[0].VpcId' \
            --output text 2>/dev/null || echo "None")

          if [ "$vpc_id" != "None" ] && [ -n "$vpc_id" ]; then
            echo "::warning::VPC still exists: $vpc_id"
            WARNINGS="${WARNINGS}\n- VPC: $vpc_id"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ VPC clean"
          fi

          echo ""
          echo "================================"
          if [ $ORPHANED -gt 0 ]; then
            echo "::warning::Found $ORPHANED orphaned resource(s) after cleanup"
            echo -e "Resources still present:$WARNINGS"
            echo ""
            echo "These resources may require manual cleanup or take time to fully delete."
            echo "Re-run destroy-all or run cleanup script manually if needed:"
            echo "  ./scripts/cleanup-aws-resources.sh ${{ github.event.inputs.environment }}"
          else
            echo "✅ All resources successfully deleted!"
          fi
          echo "================================"

  # ---------------------------------------------------------------------------
  # Summary
  # ---------------------------------------------------------------------------
  summary:
    name: Destruction Summary
    runs-on: ubuntu-latest
    needs: [destroy-app, destroy-lambda, destroy-database, destroy-eks-addons, clean-k8s-resources, destroy-eks-cluster, force-cleanup, validate-cleanup]
    if: always()
    steps:
      - name: Generate Summary
        run: |
          echo "# Infrastructure Destruction Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Environment: ${{ github.event.inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| K8s Application | ${{ needs.destroy-app.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Lambda Functions | ${{ needs.destroy-lambda.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database (RDS) | ${{ needs.destroy-database.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| K8s Addons (Phase 2) | ${{ needs.destroy-eks-addons.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **K8s Resources Cleanup** | ${{ needs.clean-k8s-resources.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| EKS Cluster (Phase 1) | ${{ needs.destroy-eks-cluster.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Force Cleanup** | ${{ needs.force-cleanup.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Final Validation** | ${{ needs.validate-cleanup.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Notes" >> $GITHUB_STEP_SUMMARY
          echo "- Force cleanup removes orphaned resources (ECR, IAM roles, etc.)" >> $GITHUB_STEP_SUMMARY
          echo "- Some resources may take time to fully delete (EKS, VPC)" >> $GITHUB_STEP_SUMMARY
          echo "- Re-run destroy-all if validation shows orphaned resources" >> $GITHUB_STEP_SUMMARY
