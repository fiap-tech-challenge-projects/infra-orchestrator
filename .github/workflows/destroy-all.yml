name: Destroy All Infrastructure

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to destroy'
        required: true
        type: choice
        options:
          - development
          - production
      confirm:
        description: 'Type "DESTROY" to confirm'
        required: true
        type: string

env:
  AWS_REGION: 'us-east-1'

jobs:
  # ---------------------------------------------------------------------------
  # Validate Confirmation
  # ---------------------------------------------------------------------------
  validate:
    name: Validate Confirmation
    runs-on: ubuntu-latest
    steps:
      - name: Check Confirmation
        if: ${{ github.event.inputs.confirm != 'DESTROY' }}
        run: |
          echo "::error::You must type 'DESTROY' to confirm destruction"
          exit 1

      - name: Warning
        run: |
          echo "::warning::You are about to destroy ALL infrastructure in ${{ github.event.inputs.environment }}"
          echo "## DESTRUCTION CONFIRMED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Environment: **${{ github.event.inputs.environment }}**" >> $GITHUB_STEP_SUMMARY

  # ---------------------------------------------------------------------------
  # Clean Terraform State Locks
  # ---------------------------------------------------------------------------
  clean-locks:
    name: Clean Terraform State Locks
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # AWS_SESSION_TOKEN is optional (only required for AWS Academy)
          # If not set, it will be empty and the action will ignore it
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Clean state locks
        run: |
          echo "Checking for stale Terraform state locks..."

          # Scan for locks
          locks=$(aws dynamodb scan --table-name fiap-terraform-locks --query 'Items[*].LockID.S' --output text 2>/dev/null || echo "")

          if [ -n "$locks" ]; then
            echo "Found locks to clean:"
            echo "$locks"

            # Delete each lock
            for lock in $locks; do
              if [[ "$lock" == *"${{ github.event.inputs.environment }}"* ]]; then
                echo "Deleting lock: $lock"
                aws dynamodb delete-item \
                  --table-name fiap-terraform-locks \
                  --key "{\"LockID\": {\"S\": \"$lock\"}}" 2>/dev/null || true
              fi
            done

            echo "✓ Locks cleaned"
          else
            echo "No locks found"
          fi

  # ---------------------------------------------------------------------------
  # Step 1: Destroy Phase 4 Microservices (reverse order)
  # ---------------------------------------------------------------------------
  destroy-microservices:
    name: "1. Destroy Phase 4 Microservices"
    runs-on: ubuntu-latest
    needs: [validate, clean-locks]
    continue-on-error: true
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        continue-on-error: true
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name fiap-tech-challenge-eks-${{ github.event.inputs.environment }} || \
          echo "EKS cluster not reachable - will skip kubectl cleanup"

      - name: Checkout os-service
        uses: actions/checkout@v4
        continue-on-error: true
        with:
          repository: ${{ github.repository_owner }}/os-service
          ref: ${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}
          token: ${{ secrets.GH_PAT }}
          path: os-service

      - name: Checkout billing-service
        uses: actions/checkout@v4
        continue-on-error: true
        with:
          repository: ${{ github.repository_owner }}/billing-service
          ref: ${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}
          token: ${{ secrets.GH_PAT }}
          path: billing-service

      - name: Checkout execution-service
        uses: actions/checkout@v4
        continue-on-error: true
        with:
          repository: ${{ github.repository_owner }}/execution-service
          ref: ${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}
          token: ${{ secrets.GH_PAT }}
          path: execution-service

      - name: Delete Microservice K8s Resources
        continue-on-error: true
        run: |
          OVERLAY="${{ github.event.inputs.environment }}"
          NAMESPACE="ftc-app-${OVERLAY}"

          for service in execution-service billing-service os-service; do
            echo "=== Deleting $service ==="
            if [ -d "$service/k8s/overlays/$OVERLAY" ]; then
              kubectl delete -k $service/k8s/overlays/$OVERLAY \
                --ignore-not-found=true --timeout=120s || true
            fi
          done

          # Delete migration jobs
          kubectl delete job os-service-migration -n $NAMESPACE --ignore-not-found=true || true

          # Delete AWS credential secrets
          for service in os-service billing-service execution-service; do
            kubectl delete secret ${service}-aws-creds -n $NAMESPACE --ignore-not-found=true || true
            kubectl delete secret dev-${service}-aws-creds -n $NAMESPACE --ignore-not-found=true || true
          done

          echo "Microservice resources deleted"

      - name: Delete ECR Repositories
        continue-on-error: true
        run: |
          for repo in os-service billing-service execution-service; do
            if aws ecr describe-repositories --repository-names "$repo" --region ${{ env.AWS_REGION }} 2>/dev/null; then
              aws ecr delete-repository --repository-name "$repo" --force --region ${{ env.AWS_REGION }}
              echo "Deleted ECR repo: $repo"
            fi
          done

  # ---------------------------------------------------------------------------
  # Step 2: Destroy Lambda Functions
  # ---------------------------------------------------------------------------
  destroy-lambda:
    name: "2. Destroy Lambda Functions"
    runs-on: ubuntu-latest
    needs: [validate, destroy-microservices]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger lambda-api-handler destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'lambda-api-handler',
                workflow_id: 'deploy.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger lambda-api-handler destroy:', error.message);
            }

      - name: Wait for Lambda destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            const dispatchTime = new Date().toISOString();
            let attempts = 0;
            await delay(15000);
            while (attempts < 20) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'lambda-api-handler',
                workflow_id: 'deploy.yml',
                created: `>=${dispatchTime.split('T')[0]}`,
                per_page: 5
              });
              const latestRun = runs.data.workflow_runs.find(r => r.created_at >= dispatchTime);
              if (!latestRun) { attempts++; await delay(15000); continue; }
              if (latestRun.status === 'completed') {
                console.log(latestRun.conclusion === 'success'
                  ? 'Lambda destruction completed successfully'
                  : `Lambda destruction failed: ${latestRun.conclusion}`);
                return;
              }
              attempts++;
              await delay(30000);
            }
            console.log('Lambda destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 2.5: Destroy Messaging Infrastructure
  # ---------------------------------------------------------------------------
  destroy-messaging:
    name: "2.5. Destroy Messaging Infrastructure"
    runs-on: ubuntu-latest
    needs: [validate, destroy-lambda]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger messaging-infra destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'messaging-infra',
                workflow_id: 'terraform.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger messaging-infra destroy:', error.message);
            }

      - name: Wait for Messaging destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            const dispatchTime = new Date().toISOString();
            let attempts = 0;
            await delay(15000);
            while (attempts < 20) {
              try {
                const runs = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: 'messaging-infra',
                  workflow_id: 'terraform.yml',
                  created: `>=${dispatchTime.split('T')[0]}`,
                  per_page: 5
                });
                const latestRun = runs.data.workflow_runs.find(r => r.created_at >= dispatchTime);
                if (!latestRun) { attempts++; await delay(15000); continue; }
                if (latestRun.status === 'completed') {
                  console.log(latestRun.conclusion === 'success'
                    ? 'Messaging destruction completed successfully'
                    : `Messaging destruction failed: ${latestRun.conclusion}`);
                  return;
                }
              } catch (error) {
                console.log('Could not check messaging-infra status:', error.message);
                return;
              }
              attempts++;
              await delay(30000);
            }
            console.log('Messaging destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 3: Destroy Database
  # ---------------------------------------------------------------------------
  destroy-database:
    name: "3. Destroy Database"
    runs-on: ubuntu-latest
    needs: [validate, destroy-messaging]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger database-managed-infra destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'database-managed-infra',
                workflow_id: 'terraform.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger database-managed-infra destroy:', error.message);
            }

      - name: Wait for Database destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            const dispatchTime = new Date().toISOString();
            let attempts = 0;
            await delay(15000);
            while (attempts < 30) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'database-managed-infra',
                workflow_id: 'terraform.yml',
                created: `>=${dispatchTime.split('T')[0]}`,
                per_page: 5
              });
              const latestRun = runs.data.workflow_runs.find(r => r.created_at >= dispatchTime);
              if (!latestRun) { attempts++; await delay(15000); continue; }
              if (latestRun.status === 'completed') {
                console.log(latestRun.conclusion === 'success'
                  ? 'Database destruction completed successfully'
                  : `Database destruction failed: ${latestRun.conclusion}`);
                return;
              }
              attempts++;
              await delay(30000);
            }
            console.log('Database destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 4a: Destroy K8s Addons (Phase 2)
  # ---------------------------------------------------------------------------
  destroy-eks-addons:
    name: "4a. Destroy K8s Addons (Phase 2)"
    runs-on: ubuntu-latest
    needs: [validate, destroy-database]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger kubernetes-addons destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'kubernetes-addons',
                workflow_id: 'terraform.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger kubernetes-addons destroy:', error.message);
            }

      - name: Wait for K8s addons destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            const dispatchTime = new Date().toISOString();
            let attempts = 0;
            await delay(15000);
            while (attempts < 20) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'kubernetes-addons',
                workflow_id: 'terraform.yml',
                created: `>=${dispatchTime.split('T')[0]}`,
                per_page: 5
              });
              const latestRun = runs.data.workflow_runs.find(r => r.created_at >= dispatchTime);
              if (!latestRun) { attempts++; await delay(15000); continue; }
              if (latestRun.status === 'completed') {
                console.log(latestRun.conclusion === 'success'
                  ? 'K8s addons destruction completed successfully'
                  : `K8s addons destruction failed: ${latestRun.conclusion}`);
                return;
              }
              attempts++;
              await delay(30000);
            }
            console.log('K8s addons destruction timed out');

  # ---------------------------------------------------------------------------
  # Step 4a.5: Clean Kubernetes Resources (before cluster destruction)
  # ---------------------------------------------------------------------------
  clean-k8s-resources:
    name: "4a.5. Clean Kubernetes Resources"
    runs-on: ubuntu-latest
    needs: [validate, destroy-eks-addons]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        continue-on-error: true
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name fiap-tech-challenge-eks-${{ github.event.inputs.environment }}

      - name: Delete ExternalSecrets and SecretStore
        continue-on-error: true
        run: |
          echo "=== Deleting ExternalSecrets and SecretStore ==="

          for namespace in ftc-app-${{ github.event.inputs.environment }}; do
            kubectl delete externalsecrets --all -n "$namespace" --timeout=30s 2>/dev/null || true
            kubectl delete secretstore --all -n "$namespace" --timeout=30s 2>/dev/null || true
            kubectl delete secret fiap-tech-challenge-db-secrets -n "$namespace" --ignore-not-found=true 2>/dev/null || true
            kubectl delete secret fiap-tech-challenge-auth-config -n "$namespace" --ignore-not-found=true 2>/dev/null || true
          done

          echo "ExternalSecrets and SecretStore deleted"

      - name: Delete all remaining resources in namespaces
        continue-on-error: true
        run: |
          echo "=== Deleting remaining resources ==="

          for namespace in ftc-app-${{ github.event.inputs.environment }}; do
            # Delete all deployments, services, ingresses, jobs, etc.
            kubectl delete all --all -n "$namespace" --timeout=120s 2>/dev/null || true
            kubectl delete ingress --all -n "$namespace" --timeout=60s 2>/dev/null || true
            kubectl delete configmap --all -n "$namespace" --timeout=30s 2>/dev/null || true
            kubectl delete secret --all -n "$namespace" --timeout=30s 2>/dev/null || true
          done

          echo "Remaining resources deleted"

      - name: Delete application namespaces
        continue-on-error: true
        run: |
          echo "=== Deleting application namespaces ==="

          for namespace in ftc-app-${{ github.event.inputs.environment }} signoz; do
            kubectl delete namespace "$namespace" \
              --ignore-not-found=true \
              --timeout=120s 2>/dev/null || true
          done

          echo "Namespaces deleted"

      - name: Wait for finalizers
        continue-on-error: true
        run: |
          echo "Waiting 30s for Kubernetes finalizers to complete..."
          sleep 30

  # ---------------------------------------------------------------------------
  # Step 4b: Destroy EKS Cluster (Phase 1)
  # ---------------------------------------------------------------------------
  destroy-eks-cluster:
    name: "4b. Destroy EKS Cluster (Phase 1)"
    runs-on: ubuntu-latest
    needs: [validate, destroy-database, destroy-eks-addons, clean-k8s-resources]
    if: always() && needs.validate.result == 'success'
    continue-on-error: true
    steps:
      - name: Trigger kubernetes-core-infra destroy
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: 'kubernetes-core-infra',
                workflow_id: 'terraform.yml',
                ref: '${{ github.event.inputs.environment == 'production' && 'main' || 'develop' }}',
                inputs: {
                  environment: '${{ github.event.inputs.environment }}',
                  action: 'destroy'
                }
              });
            } catch (error) {
              console.log('Could not trigger kubernetes-core-infra destroy:', error.message);
            }

      - name: Wait for EKS cluster destruction
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT }}
          script: |
            const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
            const dispatchTime = new Date().toISOString();
            let attempts = 0;
            await delay(15000);
            while (attempts < 60) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: 'kubernetes-core-infra',
                workflow_id: 'terraform.yml',
                created: `>=${dispatchTime.split('T')[0]}`,
                per_page: 5
              });
              const latestRun = runs.data.workflow_runs.find(r => r.created_at >= dispatchTime);
              if (!latestRun) { attempts++; await delay(15000); continue; }
              if (latestRun.status === 'completed') {
                console.log(latestRun.conclusion === 'success'
                  ? 'EKS cluster destruction completed successfully'
                  : `EKS cluster destruction failed: ${latestRun.conclusion}`);
                return;
              }
              attempts++;
              await delay(30000);
            }
            console.log('EKS cluster destruction timed out');

  # ---------------------------------------------------------------------------
  # Force Cleanup - Aggressive orphan resource cleanup
  # ---------------------------------------------------------------------------
  force-cleanup:
    name: Force Cleanup Orphaned Resources
    runs-on: ubuntu-latest
    needs: [destroy-microservices, destroy-lambda, destroy-messaging, destroy-database, destroy-eks-addons, clean-k8s-resources, destroy-eks-cluster]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install dependencies
        run: |
          sudo apt-get update -qq && sudo apt-get install -y jq > /dev/null 2>&1

      - name: Run aggressive cleanup script
        continue-on-error: true
        run: |
          chmod +x scripts/cleanup-aws-resources.sh
          echo "DELETE" | ./scripts/cleanup-aws-resources.sh ${{ github.event.inputs.environment }} || true

      - name: Force delete orphaned IAM policies
        continue-on-error: true
        run: |
          echo "=== Force deleting orphaned IAM policies ==="

          # List all custom policies with project name
          policies=$(aws iam list-policies --scope Local \
            --query "Policies[?contains(PolicyName, 'fiap-tech-challenge')].Arn" \
            --output text 2>/dev/null || echo "")

          if [ -n "$policies" ]; then
            for policy_arn in $policies; do
              policy_name=$(echo "$policy_arn" | awk -F'/' '{print $NF}')
              echo "Processing policy: $policy_name"

              # Detach from all roles
              attached_roles=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter Role --query 'PolicyRoles[*].RoleName' --output text 2>/dev/null || echo "")
              for role in $attached_roles; do
                echo "  Detaching from role: $role"
                aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Detach from all users
              attached_users=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter User --query 'PolicyUsers[*].UserName' --output text 2>/dev/null || echo "")
              for user in $attached_users; do
                echo "  Detaching from user: $user"
                aws iam detach-user-policy --user-name "$user" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Detach from all groups
              attached_groups=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" \
                --entity-filter Group --query 'PolicyGroups[*].GroupName' --output text 2>/dev/null || echo "")
              for group in $attached_groups; do
                echo "  Detaching from group: $group"
                aws iam detach-group-policy --group-name "$group" --policy-arn "$policy_arn" 2>/dev/null || true
              done

              # Delete all non-default versions
              versions=$(aws iam list-policy-versions --policy-arn "$policy_arn" \
                --query 'Versions[?!IsDefaultVersion].VersionId' --output text 2>/dev/null || echo "")
              for version in $versions; do
                aws iam delete-policy-version --policy-arn "$policy_arn" --version-id "$version" 2>/dev/null || true
              done

              # Delete the policy
              echo "  Deleting policy: $policy_name"
              aws iam delete-policy --policy-arn "$policy_arn" 2>/dev/null || true
            done
            echo "✓ IAM policies cleaned"
          else
            echo "No orphaned policies found"
          fi

  # ---------------------------------------------------------------------------
  # Validate Resources Deleted
  # ---------------------------------------------------------------------------
  validate-cleanup:
    name: Validate Resources Deleted
    runs-on: ubuntu-latest
    needs: [force-cleanup]
    if: always()
    continue-on-error: true
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install jq
        run: |
          sudo apt-get update -qq && sudo apt-get install -y jq > /dev/null 2>&1

      - name: Final validation
        run: |
          echo "=== Final Validation ==="
          echo ""

          ORPHANED=0
          WARNINGS=""

          # Check ECR
          echo "Checking ECR repositories..."
          if ecr_repos=$(aws ecr describe-repositories --region us-east-1 2>/dev/null | jq -r '.repositories[].repositoryName' | grep "fiap-tech-challenge" 2>/dev/null); then
            echo "::warning::ECR repositories still exist: $ecr_repos"
            WARNINGS="${WARNINGS}\n- ECR repositories: $ecr_repos"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ ECR clean"
          fi

          # Check EKS
          echo "Checking EKS cluster..."
          if aws eks describe-cluster --name fiap-tech-challenge-eks-${{ github.event.inputs.environment }} --region us-east-1 &>/dev/null; then
            echo "::warning::EKS cluster still exists: fiap-tech-challenge-eks-${{ github.event.inputs.environment }}"
            WARNINGS="${WARNINGS}\n- EKS cluster: fiap-tech-challenge-eks-${{ github.event.inputs.environment }}"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ EKS clean"
          fi

          # Check RDS
          echo "Checking RDS instances..."
          if aws rds describe-db-instances --db-instance-identifier fiap-tech-challenge-db-${{ github.event.inputs.environment }} --region us-east-1 &>/dev/null; then
            echo "::warning::RDS still exists: fiap-tech-challenge-db-${{ github.event.inputs.environment }}"
            WARNINGS="${WARNINGS}\n- RDS: fiap-tech-challenge-db-${{ github.event.inputs.environment }}"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ RDS clean"
          fi

          # Check IAM Roles
          echo "Checking IAM roles..."
          if iam_roles=$(aws iam list-roles 2>/dev/null | jq -r '.Roles[].RoleName' | grep "fiap-tech-challenge" 2>/dev/null); then
            echo "::warning::IAM roles still exist"
            WARNINGS="${WARNINGS}\n- IAM roles: $(echo $iam_roles | head -n 3)"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ IAM clean"
          fi

          # Check VPC
          echo "Checking VPCs..."
          vpc_id=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Project,Values=fiap-tech-challenge" \
            --region us-east-1 \
            --query 'Vpcs[0].VpcId' \
            --output text 2>/dev/null || echo "None")

          if [ "$vpc_id" != "None" ] && [ -n "$vpc_id" ]; then
            echo "::warning::VPC still exists: $vpc_id"
            WARNINGS="${WARNINGS}\n- VPC: $vpc_id"
            ORPHANED=$((ORPHANED + 1))
          else
            echo "✓ VPC clean"
          fi

          echo ""
          echo "================================"
          if [ $ORPHANED -gt 0 ]; then
            echo "::warning::Found $ORPHANED orphaned resource(s) after cleanup"
            echo -e "Resources still present:$WARNINGS"
            echo ""
            echo "These resources may require manual cleanup or take time to fully delete."
            echo "Re-run destroy-all or run cleanup script manually if needed:"
            echo "  ./scripts/cleanup-aws-resources.sh ${{ github.event.inputs.environment }}"
          else
            echo "✅ All resources successfully deleted!"
          fi
          echo "================================"

  # ---------------------------------------------------------------------------
  # Summary
  # ---------------------------------------------------------------------------
  summary:
    name: Destruction Summary
    runs-on: ubuntu-latest
    needs: [destroy-microservices, destroy-lambda, destroy-messaging, destroy-database, destroy-eks-addons, clean-k8s-resources, destroy-eks-cluster, force-cleanup, validate-cleanup]
    if: always()
    steps:
      - name: Generate Summary
        run: |
          echo "# Infrastructure Destruction Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Environment: ${{ github.event.inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Microservices (OS, Billing, Execution) | ${{ needs.destroy-microservices.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Lambda Functions | ${{ needs.destroy-lambda.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Messaging (SQS/EventBridge) | ${{ needs.destroy-messaging.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database (RDS) | ${{ needs.destroy-database.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| K8s Addons (Phase 2) | ${{ needs.destroy-eks-addons.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **K8s Resources Cleanup** | ${{ needs.clean-k8s-resources.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| EKS Cluster (Phase 1) | ${{ needs.destroy-eks-cluster.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Force Cleanup** | ${{ needs.force-cleanup.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Final Validation** | ${{ needs.validate-cleanup.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Notes" >> $GITHUB_STEP_SUMMARY
          echo "- Force cleanup removes orphaned resources (ECR, IAM roles, etc.)" >> $GITHUB_STEP_SUMMARY
          echo "- Some resources may take time to fully delete (EKS, VPC)" >> $GITHUB_STEP_SUMMARY
          echo "- Re-run destroy-all if validation shows orphaned resources" >> $GITHUB_STEP_SUMMARY
